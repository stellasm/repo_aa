{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Práctico 1 \n",
    "### Clasificación sobre datos simulados. \n",
    "\n",
    "## Introducción\n",
    "Para este trabajo, hemos creado una función generadora de minions. Sobre cada minion, hemos medido 200 características que representan habilidades que poseen en distintas tareas (relacionadas al Mal).  \n",
    "\n",
    "El doctor Nefario ha ideado una fórmula para determinar si un minion es o no apto para concretar su plan para conquistar el mundo. De esta manera ha etiquetado más de 500 minions. Lamentablemente, ha perdido dicha fórmula y necesita seguir decidiendo si nuevos minions son o no aptos para su macabro plan.\n",
    "\n",
    "Es por esto que nuestro objetivo será construir clasificadores que estimen lo mejor posible la probabilidad de que nuevos minions sean o no aptos para concretar el plan de conquista y así facilitarle las cosas al doctor Nefario.\n",
    "\n",
    "Por otra parte, ya que el doctor Nefario tuvo problemas con equipos que sobreestiman sus resultados, decidió guardarse varias etiquetas extra que no compartirá con nadie, y que luego utilizará para elegir al mejor equipo, al cual contratará para (de una vez por todas) conquistar el mundo. \n",
    "\n",
    "\n",
    "En concreto:\n",
    "\n",
    "Tendrán disponible una matriz de datos $X$ de $500$ filas en donde cada fila $x^{(i)}$ representa un vector de $200$ características de cada instancia. Es decir, $\\textbf{x}^{(i)} = x_1^{(i)}, \\dots, x_{200}^{(i)}$ con $i$ entre $1$ y $500$. Además, tendrán y, un vector de $500$ posiciones con dos posibles valores: $True$ y $False$. \n",
    "\n",
    "Por otra parte, tendrán disponibles más instancias de evaluación $X_{competencia}$ sin las respectivas etiquetas que utilizaremos para evaluar sus resultados. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PREAMBULOS\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from IPython.display import display, HTML\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as  pd\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 15)\n",
    "\n",
    "pd.set_option('precision', 4)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.ensemble\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import sklearn.model_selection\n",
    "from scipy.stats import randint as sp_randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.4914</td>\n",
       "      <td>0.1644</td>\n",
       "      <td>1.2315</td>\n",
       "      <td>1.2429</td>\n",
       "      <td>1.5576</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>0.1302</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.1983</td>\n",
       "      <td>-0.0118</td>\n",
       "      <td>1.5375</td>\n",
       "      <td>-0.7727</td>\n",
       "      <td>-0.1401</td>\n",
       "      <td>2.0871</td>\n",
       "      <td>-0.8312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.2749</td>\n",
       "      <td>0.2780</td>\n",
       "      <td>-1.3108</td>\n",
       "      <td>0.6801</td>\n",
       "      <td>-0.5503</td>\n",
       "      <td>0.6359</td>\n",
       "      <td>-0.4478</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2190</td>\n",
       "      <td>-0.3190</td>\n",
       "      <td>-0.6446</td>\n",
       "      <td>-0.0061</td>\n",
       "      <td>-1.2374</td>\n",
       "      <td>-1.3291</td>\n",
       "      <td>-1.3265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.2243</td>\n",
       "      <td>-0.5710</td>\n",
       "      <td>-0.2712</td>\n",
       "      <td>-0.1328</td>\n",
       "      <td>-1.0045</td>\n",
       "      <td>0.9315</td>\n",
       "      <td>-1.4507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>-0.1989</td>\n",
       "      <td>-0.0393</td>\n",
       "      <td>-0.5866</td>\n",
       "      <td>2.2507</td>\n",
       "      <td>1.4925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5853</td>\n",
       "      <td>-0.8532</td>\n",
       "      <td>-0.2723</td>\n",
       "      <td>-0.5493</td>\n",
       "      <td>-2.9824</td>\n",
       "      <td>-0.1697</td>\n",
       "      <td>-0.0430</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6488</td>\n",
       "      <td>-0.7363</td>\n",
       "      <td>-0.8866</td>\n",
       "      <td>-1.2717</td>\n",
       "      <td>-0.1493</td>\n",
       "      <td>0.2007</td>\n",
       "      <td>-1.4820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.4155</td>\n",
       "      <td>1.4187</td>\n",
       "      <td>0.6027</td>\n",
       "      <td>-0.7993</td>\n",
       "      <td>0.2939</td>\n",
       "      <td>-0.1796</td>\n",
       "      <td>-0.7140</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1314</td>\n",
       "      <td>-0.4230</td>\n",
       "      <td>-0.2685</td>\n",
       "      <td>0.3045</td>\n",
       "      <td>-1.2245</td>\n",
       "      <td>-1.9421</td>\n",
       "      <td>1.5186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.2516</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>-1.1980</td>\n",
       "      <td>0.4577</td>\n",
       "      <td>0.9287</td>\n",
       "      <td>0.5373</td>\n",
       "      <td>0.2476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5829</td>\n",
       "      <td>-0.5494</td>\n",
       "      <td>0.4607</td>\n",
       "      <td>1.2182</td>\n",
       "      <td>0.1025</td>\n",
       "      <td>3.0034</td>\n",
       "      <td>-0.0344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.6246</td>\n",
       "      <td>-1.0590</td>\n",
       "      <td>0.9491</td>\n",
       "      <td>0.2687</td>\n",
       "      <td>0.6610</td>\n",
       "      <td>-1.6657</td>\n",
       "      <td>0.3982</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1075</td>\n",
       "      <td>0.8993</td>\n",
       "      <td>-0.4229</td>\n",
       "      <td>0.3977</td>\n",
       "      <td>-0.0808</td>\n",
       "      <td>-1.7054</td>\n",
       "      <td>-0.4786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.2677</td>\n",
       "      <td>0.1802</td>\n",
       "      <td>0.7154</td>\n",
       "      <td>0.3542</td>\n",
       "      <td>-0.9023</td>\n",
       "      <td>-1.7792</td>\n",
       "      <td>-0.0121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8491</td>\n",
       "      <td>0.7469</td>\n",
       "      <td>0.2071</td>\n",
       "      <td>-1.0090</td>\n",
       "      <td>0.3317</td>\n",
       "      <td>-1.7513</td>\n",
       "      <td>-0.5397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.7834</td>\n",
       "      <td>1.7056</td>\n",
       "      <td>0.3418</td>\n",
       "      <td>-0.8350</td>\n",
       "      <td>0.4068</td>\n",
       "      <td>0.0495</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0130</td>\n",
       "      <td>0.1483</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>-0.0020</td>\n",
       "      <td>-1.6642</td>\n",
       "      <td>2.5117</td>\n",
       "      <td>-0.0118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.4028</td>\n",
       "      <td>-0.6085</td>\n",
       "      <td>1.0845</td>\n",
       "      <td>0.1033</td>\n",
       "      <td>0.2698</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3587</td>\n",
       "      <td>-0.3121</td>\n",
       "      <td>-0.7630</td>\n",
       "      <td>0.6525</td>\n",
       "      <td>0.6161</td>\n",
       "      <td>-0.0902</td>\n",
       "      <td>-1.0215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1       2       3       4       5       6   ...       193  \\\n",
       "index                                                           ...             \n",
       "0      1.4914  0.1644  1.2315  1.2429  1.5576  0.0455  0.1302   ...   -1.1983   \n",
       "1     -0.2749  0.2780 -1.3108  0.6801 -0.5503  0.6359 -0.4478   ...    1.2190   \n",
       "2     -0.2243 -0.5710 -0.2712 -0.1328 -1.0045  0.9315 -1.4507   ...    0.9459   \n",
       "3      0.5853 -0.8532 -0.2723 -0.5493 -2.9824 -0.1697 -0.0430   ...    1.6488   \n",
       "4     -1.4155  1.4187  0.6027 -0.7993  0.2939 -0.1796 -0.7140   ...    1.1314   \n",
       "...       ...     ...     ...     ...     ...     ...     ...   ...       ...   \n",
       "495    0.2516  0.9375 -1.1980  0.4577  0.9287  0.5373  0.2476   ...    0.5829   \n",
       "496    0.6246 -1.0590  0.9491  0.2687  0.6610 -1.6657  0.3982   ...   -0.1075   \n",
       "497    0.2677  0.1802  0.7154  0.3542 -0.9023 -1.7792 -0.0121   ...    0.8491   \n",
       "498    0.1926  0.7834  1.7056  0.3418 -0.8350  0.4068  0.0495   ...   -0.0130   \n",
       "499    0.0427  0.4028 -0.6085  1.0845  0.1033  0.2698 -0.8598   ...   -0.3587   \n",
       "\n",
       "          194     195     196     197     198     199  \n",
       "index                                                  \n",
       "0     -0.0118  1.5375 -0.7727 -0.1401  2.0871 -0.8312  \n",
       "1     -0.3190 -0.6446 -0.0061 -1.2374 -1.3291 -1.3265  \n",
       "2      0.1430 -0.1989 -0.0393 -0.5866  2.2507  1.4925  \n",
       "3     -0.7363 -0.8866 -1.2717 -0.1493  0.2007 -1.4820  \n",
       "4     -0.4230 -0.2685  0.3045 -1.2245 -1.9421  1.5186  \n",
       "...       ...     ...     ...     ...     ...     ...  \n",
       "495   -0.5494  0.4607  1.2182  0.1025  3.0034 -0.0344  \n",
       "496    0.8993 -0.4229  0.3977 -0.0808 -1.7054 -0.4786  \n",
       "497    0.7469  0.2071 -1.0090  0.3317 -1.7513 -0.5397  \n",
       "498    0.1483  0.5019 -0.0020 -1.6642  2.5117 -0.0118  \n",
       "499   -0.3121 -0.7630  0.6525  0.6161 -0.0902 -1.0215  \n",
       "\n",
       "[500 rows x 200 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       output\n",
       "index        \n",
       "0           0\n",
       "1           0\n",
       "2           0\n",
       "3           0\n",
       "4           1\n",
       "...       ...\n",
       "495         1\n",
       "496         0\n",
       "497         1\n",
       "498         0\n",
       "499         0\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Carga de datos\n",
    "X = pd.read_csv(\"X.csv\", index_col=\"index\")\n",
    "y = pd.read_csv(\"y.csv\", index_col=\"index\", dtype=int)  # Cargamos los valores booleanos (True y False)\n",
    "                                                        # como números (1 y 0) para facilitar el manejo luego. \n",
    "    \n",
    "X_competencia = pd.read_csv(\"X_competencia1.csv\", index_col=\"index\")\n",
    "y_competencia_ejemplo = pd.read_csv(\"y_competencia_ejemplo.csv\", index_col=\"index\")\n",
    "display(X)\n",
    "display(y)\n",
    "\n",
    "# Descomentar si quieren ver los datos para la competencia:\n",
    "# display(X_competencia) \n",
    "# display(y_competencia_ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.2828</td>\n",
       "      <td>2.0610</td>\n",
       "      <td>1.0889</td>\n",
       "      <td>-0.7976</td>\n",
       "      <td>0.6819</td>\n",
       "      <td>-0.0531</td>\n",
       "      <td>0.3860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6734</td>\n",
       "      <td>0.3584</td>\n",
       "      <td>1.0187</td>\n",
       "      <td>1.4373</td>\n",
       "      <td>-0.7069</td>\n",
       "      <td>0.9051</td>\n",
       "      <td>-0.5420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>-1.1895</td>\n",
       "      <td>1.1622</td>\n",
       "      <td>0.4257</td>\n",
       "      <td>0.7102</td>\n",
       "      <td>0.2699</td>\n",
       "      <td>1.1044</td>\n",
       "      <td>1.7634</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.1822</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>-0.6501</td>\n",
       "      <td>-1.7537</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.4861</td>\n",
       "      <td>-0.6913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>-0.2730</td>\n",
       "      <td>-0.2018</td>\n",
       "      <td>0.4408</td>\n",
       "      <td>-1.0598</td>\n",
       "      <td>-0.3892</td>\n",
       "      <td>2.5573</td>\n",
       "      <td>0.1915</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5449</td>\n",
       "      <td>-1.1552</td>\n",
       "      <td>1.0447</td>\n",
       "      <td>-0.6057</td>\n",
       "      <td>-2.2056</td>\n",
       "      <td>-0.4326</td>\n",
       "      <td>-0.2387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.2006</td>\n",
       "      <td>0.5208</td>\n",
       "      <td>0.0377</td>\n",
       "      <td>-0.0886</td>\n",
       "      <td>1.5035</td>\n",
       "      <td>-1.6581</td>\n",
       "      <td>-1.1057</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.7424</td>\n",
       "      <td>-0.5942</td>\n",
       "      <td>0.0929</td>\n",
       "      <td>0.4501</td>\n",
       "      <td>1.7539</td>\n",
       "      <td>-0.3211</td>\n",
       "      <td>2.0208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>-0.6236</td>\n",
       "      <td>-1.2563</td>\n",
       "      <td>-0.4989</td>\n",
       "      <td>-2.3673</td>\n",
       "      <td>0.6763</td>\n",
       "      <td>-1.3346</td>\n",
       "      <td>0.9431</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5637</td>\n",
       "      <td>0.5153</td>\n",
       "      <td>0.2446</td>\n",
       "      <td>2.4462</td>\n",
       "      <td>0.3621</td>\n",
       "      <td>-0.2002</td>\n",
       "      <td>0.2214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0.1862</td>\n",
       "      <td>-0.4901</td>\n",
       "      <td>0.4467</td>\n",
       "      <td>0.0266</td>\n",
       "      <td>-0.4089</td>\n",
       "      <td>0.6472</td>\n",
       "      <td>0.3563</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4666</td>\n",
       "      <td>-1.9118</td>\n",
       "      <td>-0.0137</td>\n",
       "      <td>-0.5356</td>\n",
       "      <td>-1.5728</td>\n",
       "      <td>1.0976</td>\n",
       "      <td>-0.1557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0.3290</td>\n",
       "      <td>-0.1691</td>\n",
       "      <td>-1.0451</td>\n",
       "      <td>-1.1327</td>\n",
       "      <td>-0.7251</td>\n",
       "      <td>1.8375</td>\n",
       "      <td>1.0902</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2854</td>\n",
       "      <td>-1.6045</td>\n",
       "      <td>0.1122</td>\n",
       "      <td>-0.4087</td>\n",
       "      <td>0.8175</td>\n",
       "      <td>-0.2249</td>\n",
       "      <td>0.6526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>-1.6937</td>\n",
       "      <td>-0.1262</td>\n",
       "      <td>-0.3666</td>\n",
       "      <td>-0.5854</td>\n",
       "      <td>0.1751</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>0.0218</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4626</td>\n",
       "      <td>-1.1507</td>\n",
       "      <td>-0.1373</td>\n",
       "      <td>-0.7572</td>\n",
       "      <td>0.4323</td>\n",
       "      <td>-1.0987</td>\n",
       "      <td>-1.2257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>-0.7277</td>\n",
       "      <td>-0.3236</td>\n",
       "      <td>0.4933</td>\n",
       "      <td>-1.2207</td>\n",
       "      <td>0.5906</td>\n",
       "      <td>0.4204</td>\n",
       "      <td>0.9490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8638</td>\n",
       "      <td>0.3422</td>\n",
       "      <td>1.2087</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>-0.3822</td>\n",
       "      <td>-0.4081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>1.5570</td>\n",
       "      <td>0.1675</td>\n",
       "      <td>-0.0585</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.6065</td>\n",
       "      <td>-1.5073</td>\n",
       "      <td>-0.1564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6873</td>\n",
       "      <td>1.0780</td>\n",
       "      <td>0.3137</td>\n",
       "      <td>-2.3829</td>\n",
       "      <td>-1.0459</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>-1.7686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4499 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1       2       3       4       5       6   ...       193  \\\n",
       "index                                                           ...             \n",
       "501    0.2828  2.0610  1.0889 -0.7976  0.6819 -0.0531  0.3860   ...    0.6734   \n",
       "502   -1.1895  1.1622  0.4257  0.7102  0.2699  1.1044  1.7634   ...   -2.1822   \n",
       "503   -0.2730 -0.2018  0.4408 -1.0598 -0.3892  2.5573  0.1915   ...    2.5449   \n",
       "504    0.2006  0.5208  0.0377 -0.0886  1.5035 -1.6581 -1.1057   ...   -1.7424   \n",
       "505   -0.6236 -1.2563 -0.4989 -2.3673  0.6763 -1.3346  0.9431   ...   -0.5637   \n",
       "...       ...     ...     ...     ...     ...     ...     ...   ...       ...   \n",
       "4995   0.1862 -0.4901  0.4467  0.0266 -0.4089  0.6472  0.3563   ...   -0.4666   \n",
       "4996   0.3290 -0.1691 -1.0451 -1.1327 -0.7251  1.8375  1.0902   ...    1.2854   \n",
       "4997  -1.6937 -0.1262 -0.3666 -0.5854  0.1751  0.0770  0.0218   ...   -0.4626   \n",
       "4998  -0.7277 -0.3236  0.4933 -1.2207  0.5906  0.4204  0.9490   ...    0.8638   \n",
       "4999   1.5570  0.1675 -0.0585  0.0094  0.6065 -1.5073 -0.1564   ...    0.6873   \n",
       "\n",
       "          194     195     196     197     198     199  \n",
       "index                                                  \n",
       "501    0.3584  1.0187  1.4373 -0.7069  0.9051 -0.5420  \n",
       "502    0.0875 -0.6501 -1.7537  0.6584 -0.4861 -0.6913  \n",
       "503   -1.1552  1.0447 -0.6057 -2.2056 -0.4326 -0.2387  \n",
       "504   -0.5942  0.0929  0.4501  1.7539 -0.3211  2.0208  \n",
       "505    0.5153  0.2446  2.4462  0.3621 -0.2002  0.2214  \n",
       "...       ...     ...     ...     ...     ...     ...  \n",
       "4995  -1.9118 -0.0137 -0.5356 -1.5728  1.0976 -0.1557  \n",
       "4996  -1.6045  0.1122 -0.4087  0.8175 -0.2249  0.6526  \n",
       "4997  -1.1507 -0.1373 -0.7572  0.4323 -1.0987 -1.2257  \n",
       "4998   0.3422  1.2087 -0.6441  0.0260 -0.3822 -0.4081  \n",
       "4999   1.0780  0.3137 -2.3829 -1.0459  0.0963 -1.7686  \n",
       "\n",
       "[4499 rows x 200 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_competencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c8fdd4fb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.array(y_competencia_ejemplo))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "### Separación de datos\n",
    "\n",
    "Contarán con una cantidad limitada de datos, por lo cual es importante tomar una buena decisión en el momento de empezar a utilizarlos. En este punto pedimos que evalúen cómo separar sus datos para desarrollo y para evaluación tomando en cuenta la competencia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_dev: (400, 200), y_dev: (400, 1) para desarrollo\n",
      "X_eval: (100, 200), y_eval: (100, 1) para evaluación\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAloAAADSCAYAAACSJ252AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c88039cef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EJERCICIO 1. \n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "\n",
    "########################################################\n",
    "## AQUI VA SU CODIGO\n",
    "# X_dev, X_eval, y_dev, y_eval = X, X, y, y  # cambiar esta línea si lo consideran necesario\n",
    "\n",
    "X_dev, X_eval, y_dev, y_eval = sklearn.model_selection.train_test_split(X, y, test_size=0.2)\n",
    "#X_dev, X_eval, y_dev, y_eval = sklearn.model_selection.train_test_split(X, y, stratify=y, test_size=0.2)\n",
    "\n",
    "# Objetivo: variables X_dev, X_eval, y_dev e y_eval asignadas\n",
    "#########################################################\n",
    "\n",
    "\n",
    "print(\"X_dev: {}, y_dev: {} para desarrollo\".format(X_dev.shape, y_dev.shape))\n",
    "print(\"X_eval: {}, y_eval: {} para evaluación\".format(X_eval.shape, y_eval.shape))\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(np.array(y_dev))  # muestra un histograma para la distribución de y.\n",
    "plt.title('Dev Set')\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(np.array(y_eval))  # muestra un histograma para la distribución de y.\n",
    "plt.title('Hold-out Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siempre los esta separando igual. Esto es porque el np.random.seed(1234) me fija el random state. Si lo comento, cambia. Cuando lo cambio, aleatoriamente me pueden quedar balanceadas distinto las poblaciones. Eso se puede prefijar con el atributo stratify del split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>-1.1195</td>\n",
       "      <td>0.6030</td>\n",
       "      <td>-0.7158</td>\n",
       "      <td>-0.3789</td>\n",
       "      <td>-0.1388</td>\n",
       "      <td>0.7073</td>\n",
       "      <td>-0.7359</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0288</td>\n",
       "      <td>-0.7579</td>\n",
       "      <td>-0.7834</td>\n",
       "      <td>-0.5957</td>\n",
       "      <td>-0.7153</td>\n",
       "      <td>-1.3174</td>\n",
       "      <td>0.7091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1.0271</td>\n",
       "      <td>1.1688</td>\n",
       "      <td>0.5346</td>\n",
       "      <td>-1.4661</td>\n",
       "      <td>1.3467</td>\n",
       "      <td>-0.8582</td>\n",
       "      <td>0.7915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6228</td>\n",
       "      <td>1.0473</td>\n",
       "      <td>2.0603</td>\n",
       "      <td>-0.6190</td>\n",
       "      <td>1.6026</td>\n",
       "      <td>0.3663</td>\n",
       "      <td>-0.4242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>-0.8613</td>\n",
       "      <td>1.2813</td>\n",
       "      <td>-0.5670</td>\n",
       "      <td>0.5977</td>\n",
       "      <td>-0.5429</td>\n",
       "      <td>-0.6271</td>\n",
       "      <td>-0.3877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0876</td>\n",
       "      <td>1.2859</td>\n",
       "      <td>0.2696</td>\n",
       "      <td>-0.3639</td>\n",
       "      <td>0.5137</td>\n",
       "      <td>0.5838</td>\n",
       "      <td>0.7064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>0.2907</td>\n",
       "      <td>1.0031</td>\n",
       "      <td>-0.5272</td>\n",
       "      <td>0.8402</td>\n",
       "      <td>-1.2717</td>\n",
       "      <td>-1.1037</td>\n",
       "      <td>0.8865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8659</td>\n",
       "      <td>-1.3016</td>\n",
       "      <td>-1.5818</td>\n",
       "      <td>-1.0231</td>\n",
       "      <td>-1.5029</td>\n",
       "      <td>0.3956</td>\n",
       "      <td>-1.5723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1.5610</td>\n",
       "      <td>-1.4369</td>\n",
       "      <td>0.6733</td>\n",
       "      <td>0.3506</td>\n",
       "      <td>1.2520</td>\n",
       "      <td>0.1941</td>\n",
       "      <td>0.5873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2312</td>\n",
       "      <td>1.3822</td>\n",
       "      <td>-0.3384</td>\n",
       "      <td>-0.6597</td>\n",
       "      <td>1.2298</td>\n",
       "      <td>0.9270</td>\n",
       "      <td>2.1434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>-0.6085</td>\n",
       "      <td>-0.3880</td>\n",
       "      <td>0.2325</td>\n",
       "      <td>-2.7648</td>\n",
       "      <td>-0.0115</td>\n",
       "      <td>-0.3268</td>\n",
       "      <td>-0.5620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5442</td>\n",
       "      <td>1.0271</td>\n",
       "      <td>0.4207</td>\n",
       "      <td>1.2314</td>\n",
       "      <td>-0.7099</td>\n",
       "      <td>0.6231</td>\n",
       "      <td>-0.2573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.5195</td>\n",
       "      <td>0.8877</td>\n",
       "      <td>0.8757</td>\n",
       "      <td>0.7548</td>\n",
       "      <td>0.7840</td>\n",
       "      <td>-1.1044</td>\n",
       "      <td>0.3763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9216</td>\n",
       "      <td>0.8132</td>\n",
       "      <td>0.4020</td>\n",
       "      <td>0.1848</td>\n",
       "      <td>-1.0342</td>\n",
       "      <td>0.8420</td>\n",
       "      <td>-0.3683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>-0.1442</td>\n",
       "      <td>1.4870</td>\n",
       "      <td>0.4333</td>\n",
       "      <td>0.7692</td>\n",
       "      <td>0.3280</td>\n",
       "      <td>1.6126</td>\n",
       "      <td>0.5330</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1398</td>\n",
       "      <td>-0.8055</td>\n",
       "      <td>0.2608</td>\n",
       "      <td>0.3259</td>\n",
       "      <td>0.1635</td>\n",
       "      <td>0.4384</td>\n",
       "      <td>0.4438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>0.9136</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.4547</td>\n",
       "      <td>1.7563</td>\n",
       "      <td>-0.3193</td>\n",
       "      <td>-0.3693</td>\n",
       "      <td>0.2745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5628</td>\n",
       "      <td>0.1046</td>\n",
       "      <td>-0.6227</td>\n",
       "      <td>0.5660</td>\n",
       "      <td>1.6511</td>\n",
       "      <td>0.1009</td>\n",
       "      <td>-0.4971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>-0.4651</td>\n",
       "      <td>-0.0443</td>\n",
       "      <td>0.9159</td>\n",
       "      <td>-0.6645</td>\n",
       "      <td>-0.2216</td>\n",
       "      <td>-0.4516</td>\n",
       "      <td>0.1798</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7760</td>\n",
       "      <td>-2.5692</td>\n",
       "      <td>-0.5418</td>\n",
       "      <td>-1.3497</td>\n",
       "      <td>-0.5983</td>\n",
       "      <td>-0.3920</td>\n",
       "      <td>-1.0995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       1       2       3       4       5       6   ...       193  \\\n",
       "index                                                           ...             \n",
       "67    -1.1195  0.6030 -0.7158 -0.3789 -0.1388  0.7073 -0.7359   ...   -1.0288   \n",
       "416    1.0271  1.1688  0.5346 -1.4661  1.3467 -0.8582  0.7915   ...    0.6228   \n",
       "350   -0.8613  1.2813 -0.5670  0.5977 -0.5429 -0.6271 -0.3877   ...    0.0876   \n",
       "358    0.2907  1.0031 -0.5272  0.8402 -1.2717 -1.1037  0.8865   ...    0.8659   \n",
       "112    1.5610 -1.4369  0.6733  0.3506  1.2520  0.1941  0.5873   ...    0.2312   \n",
       "...       ...     ...     ...     ...     ...     ...     ...   ...       ...   \n",
       "214   -0.6085 -0.3880  0.2325 -2.7648 -0.0115 -0.3268 -0.5620   ...    0.5442   \n",
       "181    0.5195  0.8877  0.8757  0.7548  0.7840 -1.1044  0.3763   ...    0.9216   \n",
       "386   -0.1442  1.4870  0.4333  0.7692  0.3280  1.6126  0.5330   ...   -0.1398   \n",
       "407    0.9136  0.1590  0.4547  1.7563 -0.3193 -0.3693  0.2745   ...    0.5628   \n",
       "435   -0.4651 -0.0443  0.9159 -0.6645 -0.2216 -0.4516  0.1798   ...   -0.7760   \n",
       "\n",
       "          194     195     196     197     198     199  \n",
       "index                                                  \n",
       "67    -0.7579 -0.7834 -0.5957 -0.7153 -1.3174  0.7091  \n",
       "416    1.0473  2.0603 -0.6190  1.6026  0.3663 -0.4242  \n",
       "350    1.2859  0.2696 -0.3639  0.5137  0.5838  0.7064  \n",
       "358   -1.3016 -1.5818 -1.0231 -1.5029  0.3956 -1.5723  \n",
       "112    1.3822 -0.3384 -0.6597  1.2298  0.9270  2.1434  \n",
       "...       ...     ...     ...     ...     ...     ...  \n",
       "214    1.0271  0.4207  1.2314 -0.7099  0.6231 -0.2573  \n",
       "181    0.8132  0.4020  0.1848 -1.0342  0.8420 -0.3683  \n",
       "386   -0.8055  0.2608  0.3259  0.1635  0.4384  0.4438  \n",
       "407    0.1046 -0.6227  0.5660  1.6511  0.1009 -0.4971  \n",
       "435   -2.5692 -0.5418 -1.3497 -0.5983 -0.3920 -1.0995  \n",
       "\n",
       "[100 rows x 200 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probar de splitear para distintos random states con el stratify y sin. \n",
    "\n",
    "Habria que ver que pasaria si usaran un balance distinto de datos para la competencia. O si el balance de los datos en el entrenamiento te condiciona mucho. Si los datos estan muy desbalanceados, es mas estable el auc de la curva precision recall? Igual los datos no estan casi desbalanceados. Pero si lo estuvieran, habria que ver si se le puede dar mas peso a la clase minoria, como pasa con logistic regression con el weight class. Aunque quizas splitear a ciegas es mejor para no meter proporcionalidad. Se podria ver como cambia el auc usando distintos grupos de hold out. O sea, el hold out total de 0.2 se lo puede dividir en 1 hold out de 0.1 y lo restante separarlo en 2 grupos completamente desbalanceados o en 3, agregando uno balanceado pero menor. De esa manera puedo hacer un análisis mas profundo para predecir cuanto va a dar la roc auc. Aunque el tema ahi es que pierdo tamaño del set. Y el X competencia es largo, tiene 4500 datos. Pero no sabemos si esta balanceado porque el y competencia que dieron es de ejemplo y el real lo tienen ellos.\n",
    "\n",
    "Tambien, para predecir mejor que tan generalizable es el modelo, en lugar de separar el hold out set en varios independientes, se puede hacer bootstrapping. Manteniendo el mismo split y el mismo modelo, se pueden hacer subsets random del dev set y del holdout set y medir las aucs. Ahora no se va a tener un unico valor, si no que se va a tener una distribucion con su estadistica. Si defino que los subsets resampleados para el bootstrapping tengan el mismo stratify que el grupo total, voy a reducir la varianza producida por la estratificacion variable de los resamples. Pero quizas esto no es conveniente ya que en realidad no se si ellos van a usar una muestra igual de estratificada. Y deberia ser independiente de eso mi prediccion de cuan generalizable es el modelo.\n",
    "\n",
    "Otra forma de ver generalizacion es repetir el proceso completo de split entrenamiento etc. Y entonces puedo sacar una estadistica, pero no voy a tener un unico modelo. Salvo que me quede con el modelo que tenga un auc igual a la mediana. Y esto esta bueno para ver como que tan distintos van a ser los modelos segun si elegi un split random especifico de casualidad u otro. Entonces puedo estudiar robustez. Y si me quedo con el modelo de la mediana, me puedo fijar la performance en train y test. Y capaz a ese modelo y split hacerle bootstrapping.\n",
    "\n",
    "Y que pasa con las probabilidades de cada instancia? van a ser estables?\n",
    "\n",
    "Otra cosa que se puede hacer es ver si el set de hold out tiene la misma info que el de entrenamiento. Para esto, se usa como \"y\" el set al que pertenece y se evalua un clasificador a ver si puede diferenciar ambos grupos. Si no puede, la info de los 2 grupos es la misma.\n",
    "\n",
    "Para el cross validation se puede hacer kfold o one leave out, que va a ser mas costoso.\n",
    "\n",
    "Revisar metodos de calibracion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "### Construcción de modelos\n",
    "\n",
    "Para este punto, la tarea consiste en construir y evaluar modelos de tipo árbol de decisión, de manera de obtener una estimación realista de la performance de los mismos. \n",
    "\n",
    "1. Entrenar un árbol de decisión con altura máxima 3 y el resto de los hiperparámetros en default. \n",
    "2. Estimar la performance del modelo utilizando K-fold cross validation con K = 5, con las métricas “Accuracy” y “ROC AUC”. Para ello, se pide medir la performance en cada partición tanto sobre el fold de validación como sobre los folds de entrenamiento. Luego, completar la primera tabla.\n",
    "3. Entrenar árboles de decisión para cada una de las siguientes combinaciones y completar la segunda tabla.\n",
    "\n",
    "----\n",
    "\n",
    "**EJERCICIO EXTRA: Usar la implementación de árboles de decisión que realizaron para la guía de ejercicios de la materia. Adaptarla para que cumpla con la interfaz requerida por sklearn, asegurarse de que funcione con variables continuas y reproducir las tablas anteriores.   **\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> TABLA 1 </h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy (training)</th>\n",
       "      <th>Accuracy (validación)</th>\n",
       "      <th>AUC ROC (training)</th>\n",
       "      <th>AUC ROC (validación)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Permutación</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8150</td>\n",
       "      <td>0.5926</td>\n",
       "      <td>0.8228</td>\n",
       "      <td>0.5971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8500</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.8516</td>\n",
       "      <td>0.6263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8063</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.7980</td>\n",
       "      <td>0.6452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8344</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.8285</td>\n",
       "      <td>0.7197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.8037</td>\n",
       "      <td>0.5949</td>\n",
       "      <td>0.7846</td>\n",
       "      <td>0.5691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy (training)  Accuracy (validación)  AUC ROC (training)  \\\n",
       "Permutación                                                                   \n",
       "1                         0.8150                 0.5926              0.8228   \n",
       "2                         0.8500                 0.6250              0.8516   \n",
       "3                         0.8063                 0.6625              0.7980   \n",
       "4                         0.8344                 0.7250              0.8285   \n",
       "5                         0.8037                 0.5949              0.7846   \n",
       "\n",
       "             AUC ROC (validación)  \n",
       "Permutación                        \n",
       "1                          0.5971  \n",
       "2                          0.6263  \n",
       "3                          0.6452  \n",
       "4                          0.7197  \n",
       "5                          0.5691  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAggAAAEGCAYAAAAAHm2OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c8804c45c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies_training = []\n",
    "accuracies_validation = []\n",
    "aucs_training = []\n",
    "aucs_validation = []\n",
    "\n",
    "np.random.seed(SEED) # esto lo agregue para fijar el random state\n",
    "\n",
    "# Puede serles de utilidad tener a X_dev e y_dev como matrices de numpy directamente:\n",
    "X_dev_np = np.array(X_dev)\n",
    "y_dev_np = np.array(y_dev).ravel()\n",
    "\n",
    "########################################################\n",
    "## AQUI VA SU CODIGO \n",
    "arbol_2_1 = DecisionTreeClassifier(max_depth=3)\n",
    "cv = sklearn.model_selection.StratifiedKFold(n_splits=5)\n",
    "splits = cv.split(X_dev, y_dev)\n",
    "#scores = pd.DataFrame(columns=['accuracies_training','accuracies_validation','aucs_training','aucs_validation'])\n",
    "for train_idxs, val_idxs in splits:\n",
    "    arbol_2_2 = DecisionTreeClassifier(max_depth=3)\n",
    "    arbol_2_2.fit(X_dev.iloc[train_idxs], y_dev.iloc[train_idxs])\n",
    "    y_train = y_dev.iloc[train_idxs]\n",
    "    y_val = y_dev.iloc[val_idxs]\n",
    "    y_train_pred = arbol_2_2.predict(X_dev.iloc[train_idxs])\n",
    "    y_val_pred = arbol_2_2.predict(X_dev.iloc[val_idxs])\n",
    "#    scores = scores.append(pd.Series({\n",
    "#        'accuracies_training': sklearn.metrics.accuracy_score(y_train, y_train_pred),\n",
    "#        'accuracies_validation': sklearn.metrics.accuracy_score(y_val, y_val_pred),\n",
    "#        'aucs_training': sklearn.metrics.roc_auc_score(y_train, y_train_pred),\n",
    "#        'aucs_validation': sklearn.metrics.roc_auc_score(y_val, y_val_pred)\n",
    "#     }), ignore_index=True)\n",
    "    accuracies_training.append(sklearn.metrics.accuracy_score(y_train, y_train_pred))\n",
    "    accuracies_validation.append(sklearn.metrics.accuracy_score(y_val, y_val_pred))\n",
    "    aucs_training.append(sklearn.metrics.roc_auc_score(y_train, y_train_pred))\n",
    "    aucs_validation.append(sklearn.metrics.roc_auc_score(y_val, y_val_pred))\n",
    "## Objetivo: accuracies_training, accuracies_validation, aucs_training y aucs_validation asignados\n",
    "#########################################################\n",
    "\n",
    "df = pd.DataFrame(index=range(1,6))\n",
    "df.index.name = \"Permutación\"\n",
    "                  \n",
    "df[\"Accuracy (training)\"] = accuracies_training     # cambiar por accuracies_training\n",
    "df[\"Accuracy (validación)\"] = accuracies_validation   # cambiar por accuracies_validation\n",
    "df[\"AUC ROC (training)\"] = aucs_training      # cambiar por aucs_training\n",
    "df[\"AUC ROC (validación)\"] = aucs_validation    # cambiar por aucs_validation\n",
    "\n",
    "\n",
    "display(HTML(\"<h3> TABLA 1 </h3>\"))\n",
    "display(df)\n",
    "\n",
    "# Descomentar las siguientes líneas para graficar el resultado\n",
    "df.plot(kind=\"bar\")\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.0, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3> TABLA 2 </h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Altura máxima</th>\n",
       "      <th>Criterio de evaluación de corte</th>\n",
       "      <th>AUC ROC promedio (training)</th>\n",
       "      <th>AUC ROC promedio (validación)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Gini</td>\n",
       "      <td>0.8044</td>\n",
       "      <td>0.6715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Gini</td>\n",
       "      <td>0.9229</td>\n",
       "      <td>0.6763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inifinito</td>\n",
       "      <td>Gini</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Ganancia de Información</td>\n",
       "      <td>0.7890</td>\n",
       "      <td>0.6514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Ganancia de Información</td>\n",
       "      <td>0.9151</td>\n",
       "      <td>0.6346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Inifinito</td>\n",
       "      <td>Ganancia de Información</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.6763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Altura máxima Criterio de evaluación de corte  AUC ROC promedio (training)  \\\n",
       "0             3                            Gini                       0.8044   \n",
       "1             5                            Gini                       0.9229   \n",
       "2     Inifinito                            Gini                       1.0000   \n",
       "3             3         Ganancia de Información                       0.7890   \n",
       "4             5         Ganancia de Información                       0.9151   \n",
       "5     Inifinito         Ganancia de Información                       1.0000   \n",
       "\n",
       "   AUC ROC promedio (validación)  \n",
       "0                         0.6715  \n",
       "1                         0.6763  \n",
       "2                         0.6579  \n",
       "3                         0.6514  \n",
       "4                         0.6346  \n",
       "5                         0.6763  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resultados_training = []\n",
    "resultados_validation = []\n",
    "\n",
    "########################################################\n",
    "## AQUI VA SU CODIGO \n",
    "## Objetivo: resultados_training y resultados_validation asignadas\n",
    "#\n",
    "## Recomendamos seguir el siguiente esquema:\n",
    "np.random.seed(SEED)\n",
    "for criterio in [\"gini\", \"entropy\"]:\n",
    "    for altura in [3, 5, None]:\n",
    "#         CODIGO AQUI.\n",
    "        arbol_2_3 = DecisionTreeClassifier(max_depth=altura, criterion=criterio)\n",
    "        arbol_2_3.fit(X_dev, y_dev)    \n",
    "        y_dev_pred = arbol_2_3.predict(X_dev)\n",
    "        y_eval_pred = arbol_2_3.predict(X_eval)\n",
    "        resultados_training.append(sklearn.metrics.roc_auc_score(y_dev, y_dev_pred))\n",
    "        resultados_validation.append(sklearn.metrics.roc_auc_score(y_eval, y_eval_pred))\n",
    "#########################################################\n",
    "\n",
    "df = pd.DataFrame(index=range(0,6))\n",
    "\n",
    "df[\"Altura máxima\"] = [3, 5, \"Inifinito\"] * 2\n",
    "df[\"Criterio de evaluación de corte\"] = [\"Gini\"] * 3 + [\"Ganancia de Información\"] * 3\n",
    "df[\"AUC ROC promedio (training)\"] = resultados_training # reemplazar por resultados_training\n",
    "df[\"AUC ROC promedio (validación)\"] = resultados_validation # reemplazar por resultados_validation\n",
    "\n",
    "   \n",
    "display(HTML(\"<h3> TABLA 2 </h3>\"))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3: Comparación de algoritmos\n",
    "\n",
    "\n",
    "Se pide explorar distintas combinaciones de algoritmos de aprendizaje e hiperparámetros, de manera de buscar una performance óptima. Para este ejercicio es necesario que evalúen posibilidades utilizando la técnica de Grid Search. Como métrica de performance, usar siempre el área bajo la curva (AUC ROC) resultante de 5-fold cross-validation. \n",
    "\n",
    "Algoritmos a probar: KNN, árboles de decisión, LDA, Naive Bayes y SVM. Hiperparámetros: Revisar la documentación de cada uno para la búsqueda de combinaciones prometedoras.  \n",
    "\n",
    "Se pide generar un reporte que contenga: \n",
    "\n",
    "1. Una descripción de las distintas combinaciones consideradas y su performance asociada (las que consideren relevantes, con al menos la mejor combinación para cada algoritmo). \n",
    "\n",
    "1. Una breve explicación de los factores que creen que produjeron dicho resultado. \n",
    "\n",
    "En este punto evaluaremos tanto los hiperparámetros elegidos como las conclusiones relacionadas a por qué piensan que ciertos algoritmos funcionan mejor que otros para estos datos. \n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "**EJERCICIO EXTRA**: Utilizar RandomizedSearchCV con rangos de parámetros que contengan a los utilizados en el GridSearch. Analizar si se encontraron mejores combinaciones de parámetros que no hayan sido tenidas en cuenta con el GridSearch y cuál fue la diferencia de tiempo de ejecución. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def top_resultados(grid, top=5):\n",
    "    ## Si quieren, pueden utilizar esta función para imprimir las mejores combinaciones de su grid\n",
    "    print(\"Top {} combinaciones\".format(top))\n",
    "    df = pd.DataFrame(grid.cv_results_[\"params\"])\n",
    "    df[\"mean_score_validation\"] = grid.cv_results_[\"mean_test_score\"]\n",
    "    df[\"mean_score_training\"] = grid.cv_results_[\"mean_train_score\"]\n",
    "    display(df.sort_values(by=\"mean_score_validation\", ascending=False).head(top))\n",
    "\n",
    "########################################################\n",
    "## AQUI VA SU CODIGO \n",
    "## Objetivo: comparar y explorar distintas combinaciones de parámetros para los algoritmos importados arriba\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusiones Grid Search\n",
    "\n",
    "< < COMPLETAR > >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4: \n",
    "### Diagnóstico Sesgo-Varianza. \n",
    "\n",
    "En este punto, se pide inspeccionar dos de sus mejores modelos encontrados hasta ahora: el mejor modelo de tipo árbol de decisión y el mejor de tipo SVM. Para ello:\n",
    "\n",
    "1. Graficar curvas de complejidad para cada modelo, variando la profundidad en el caso de árboles, y el hiperparámetro C en el caso de SVM. Diagnosticar cómo afectan al sesgo y a la varianza esos dos hiperparámetros.\n",
    "2. Graficar curvas de aprendizaje para cada modelo. En base a estas curvas, sacar conclusiones sobre si los algoritmos parecen haber alcanzado su límite, o bien si aumentar la cantidad de datos debería ayudar.\n",
    "3. Construir un modelo RandomForest con 200 árboles. Explorar para qué sirve el hiperparámetro max_features y cómo afecta a la performance del algoritmo mediante una curva de complejidad. Explicar por qué creen que se dieron los resultados obtenidos. Por último, graficar una curva de aprendizaje sobre los parámetros elegidos para determinar si sería útil o no conseguir más datos (usar  grid search para encontrar una buena combinación de parámetros).  \n",
    "\n",
    "\n",
    "**Atención**: Tener en cuenta que debemos seguir utilizando ROC AUC como métrica para estas curvas.\n",
    "\n",
    "**ver**: http://scikit-learn.org/stable/modules/learning_curve.html#learning-curve\n",
    "\n",
    "----\n",
    "**EJERCICIO EXTRA:** Utilizar RandomizedSearchCV para explorar la performance del algoritmo de Gradient Boosting y comparar con los resultados obtenidos en el punto (c).\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competencias\n",
    "\n",
    "La entrega del trabajo estará acompañada de una competencia en la cual deberán poner a prueba su mejor modelo y sobre todo, su capacidad para estimar sus resultados. \n",
    "\n",
    "Su tarea será estimar la performance (AUC ROC) que tendrá su mejor modelo en datos de evaluación (X_competencia). \n",
    "\n",
    "Para ello, deberán predecir las probabilidades de las distintas instancias con su modelo, enviarnos dichas probabilidades junto a una estimación con 4 decimales de cuál será el AUC ROC resultante y calcularemos el resultado real. El grupo que consiga acercarse más al valor real, será el grupo ganador.  \n",
    "\n",
    "Recomendamos no perder de vista esta competencia en el momento de separar los datos en los primeros puntos. \n",
    "\n",
    "Para esto, junto con la entrega del informe, deberán enviar un archivo en formato csv con las columnas “index” y “output” (ver ejemplo de archivo en: [y_competencia_ejemplo.csv](https://github.com/pbrusco/aa-notebooks/blob/master/TP1/y_competencia_ejemplo.csv)) y un valor esperado de AUC ROC. \n",
    "\n",
    "\n",
    "## Entrega\n",
    "- Contarán con un esqueleto en formato Jupyter Notebook en donde tendrán que completar las celdas faltantes (ya sea con explicaciones y gráficos o código). \n",
    "- El notebook final deberá ser entregado en formatos .html e .ipynb. Es necesario que los resultados puedan reproducirse al ejecutar todas las celdas en orden (Kernel - Restart and Run All) utilizando las bibliotecas requeridas en el archivo: requirements.txt del repositorio. \n",
    "- Tienen tiempo hasta las 23:59hs del día miércoles 17/10/2018. La entrega se debe realizar a través del campus virtual y debe contener el informe.\n",
    "- El trabajo deberá elaborarse en grupos de 3 personas.\n",
    "- Se podrán pedir pruebas de integridad y autoría; es decir, verificar que la salida solicitada es fruto del modelo presentado y que el modelo fue construido según lo requerido en este enunciado.\n",
    "- La evaluación será grupal y se basará en la calidad del informe (presentación, claridad, prolijidad); la originalidad, practicidad y coherencia técnica de la solución; la corrección y solidez de las pruebas realizadas.\n",
    "- En el primer parcial se incluirá una pregunta sobre la solución entregada. Esa pregunta no influirá en la nota del parcial, pero sí en la nota individual del TP1.\n",
    "- La participación en la competencia es obligatoria. De todas maneras, el resultado no incidirán en la nota de la materia.\n",
    "- Los ejercicios extra son opcionales para aprobar el TP, pero son obligatorios para promocionar la materia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
